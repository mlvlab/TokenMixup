# TokenMixup

This is the official implementation of the NeurIPS 2022 paper, "[TokenMixup: Efficient Attention-guided Data Augmentation for Transformers](https://arxiv.org/abs/2210.07562)" by H. Choi, J. Choi, and H. J. Kim.

Code will be released soon :)

<!-- ⠀            |  ⠀
:-------------------------:|:-------------------------:
![htm_gif](https://raw.githubusercontent.com/mlvlab/TokenMixup/main/assets/HTM.gif)  |  ![vtm_gif](https://raw.githubusercontent.com/mlvlab/TokenMixup/main/assets/VTM.gif)  -->
<p float="middle">
  <img src="https://raw.githubusercontent.com/mlvlab/TokenMixup/main/assets/HTM.gif" width="49%" />
  <img src="https://raw.githubusercontent.com/mlvlab/TokenMixup/main/assets/VTM.gif" width="49%" /> 
</p>


## Citation

```
@inproceedings{choi2022tokenmixup,
  title={TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers},
  author={Choi, Hyeong Kyu and Choi, Joonmyung and Kim, Hyunwoo J.},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
```

## License
Code is released under [MIT License](https://github.com/mlvlab/TokenMixup/blob/main/LICENSE).

> Copyright (c) 2022-present Korea University MLV Lab
